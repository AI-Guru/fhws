{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Dr. Tristan Behrens\n",
    "## Heute: Einführung in neuronale Netze mit Hilfe objektorientierter Programmierung\n",
    "---\n",
    "<a href=\"https://colab.research.google.com/github/AI-Guru/fhws/blob/master/01%20Pflichtvortrag%20Deutsch/oop%20deep%20learning.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" style=\"margin-left:0\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kurzfassung.\n",
    "\n",
    "- Wie konstruiert man objektorientiert tiefe Neuronale Netze?\n",
    "- Was sind die Bausteine?\n",
    "- Wie ermittelt man, wie \"gut\" ein Neuronales Netz ist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tiefe Neuronale Netze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "if tf.__version__.startswith(\"1.\"):\n",
    "    tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Schichttyp: Fully connected bzw. Dense.\n",
    "\n",
    "- Einfachste Schicht.\n",
    "- Formel: `fc(x) = W * x + b`.\n",
    "- Freiheitsgrade: Weights und Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, hidden_units):\n",
    "        super(FullyConnectedLayer, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.weights_trainable = self.add_weight(\n",
    "            \"weights\",\n",
    "            shape=(int(input_shape[-1]), self.hidden_units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True)\n",
    "        self.biases_trainable = self.add_weight(\n",
    "            \"biases\",\n",
    "            shape=(self.hidden_units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.matmul(x, self.weights_trainable) + self.biases_trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Schichten können einfach instanziiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_connected_layer = FullyConnectedLayer(2)\n",
    "fully_connected_layer.build(input_shape=(3,))\n",
    "print(\"weights:\", fully_connected_layer.weights_trainable.numpy())\n",
    "print(\"biases:\", fully_connected_layer.biases_trainable.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Schichten sind mathematische Funktionen, die auf eingaben angewandt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((1, 3))\n",
    "y = fully_connected_layer(x)\n",
    "print(\"x:\", x.numpy())\n",
    "print(\"y:\", y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Mehrere Schichten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_connected_layer_1 = FullyConnectedLayer(2)\n",
    "fully_connected_layer_2 = FullyConnectedLayer(1)\n",
    "\n",
    "x = tf.ones((1, 3))\n",
    "y = fully_connected_layer_2(fully_connected_layer_1(x))\n",
    "\n",
    "print(\"x:\", x.numpy())\n",
    "print(\"y:\", y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Frage:** Ist dieses ANN wirklich tief?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Aktivierungsfunktion ReLU.\n",
    "\n",
    "- Sehr einfach zu berechnen, `relu(x) = max(0, x)`.\n",
    "- Nichtlinear.\n",
    "- Sehr gut für alle Schichten geeignet, die nicht die Ausgabeschicht sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ReluLayer, self).__init__(dtype=\"float32\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "        \n",
    "    def call(self, x):\n",
    "        return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Auch Aktivierungsfunktionen können einfach instantiiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_layer = ReluLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "So sieht ReLU aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_x = []\n",
    "points_y = []\n",
    "for x in np.arange(-5.0, 5.0, 0.125):\n",
    "    y = relu_layer([x]).numpy()[0]\n",
    "    points_x.append(x)\n",
    "    points_y.append(y)\n",
    "\n",
    "plt.scatter(points_x, points_y)\n",
    "plt.title(\"ReLU activation function.\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Aktivierungsfunktion Sigmoid.\n",
    "\n",
    "- Ein wenig aufwendiger als sigmoid.\n",
    "- Formel ist `sigmoid(x) = exp(x) / (exp(x) + 1)`.\n",
    "- Wird eher bei Ausgabeschichten benutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SigmoidLayer, self).__init__(dtype=\"float32\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "        \n",
    "    def call(self, x):\n",
    "        return tf.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Die Instanziierung ist einfach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_layer = SigmoidLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Und so sieht sigmoid aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_x = []\n",
    "points_y = []\n",
    "for x in np.arange(-5.0, 5.0, 0.125):\n",
    "    y = sigmoid_layer([x]).numpy()[0]\n",
    "    points_x.append(x)\n",
    "    points_y.append(y)\n",
    "\n",
    "plt.scatter(points_x, points_y)  \n",
    "plt.title(\"Sigmoid activation function.\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Aktivierungsfunktion Softmax.\n",
    "\n",
    "- Normalisierte Exponentialfunktion.\n",
    "- Formel: `softmax(x) = exp(x) / sum(exp(x))`.\n",
    "- Alle Ergebnisse sind zwischen 0 und 1.\n",
    "- Summe der Ergebnisse ist 1.\n",
    "- Wahrscheinlichkeitsverteilung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SoftmaxLayer, self).__init__(dtype=\"float32\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "        \n",
    "    def call(self, x):\n",
    "        return tf.nn.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Instantiierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_layer = SoftmaxLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Visualisierung von Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.0, -0.5, 0.1, 0.2, 0.0]\n",
    "y = softmax_layer(x).numpy()\n",
    "\n",
    "plt.plot(x, label=\"x\")\n",
    "plt.plot(y, label=\"y = softmax(y)\")\n",
    "plt.legend()\n",
    "plt.title(\"Softmax activation function.\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handschriftenerkennung mit einem tiefen Neuronalen Netz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Das MNIST-Dataset.\n",
    "\n",
    "- Datenbank handgeschriebener Ziffern.\n",
    "- Das \"Hallo Welt\" vom Machine Learning.\n",
    "- Anwendung im Finanz- und Postwesen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print(\"train_images\", train_images.shape)\n",
    "print(\"train_labels\", train_labels.shape)\n",
    "print(\"test_images\", test_images.shape)\n",
    "print(\"test_labels\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Frage:** Warum Training- und Testmenge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Dataset-Visualisierung (Original):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "for image, label in zip(train_images[:4], train_labels[:4]):\n",
    "    plt.subplot(1, 4, index)\n",
    "    plt.imshow(image, cmap=plt.get_cmap(\"gray\"))\n",
    "    plt.title(\"Label: {}.\".format(label))\n",
    "    index += 1 \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Multi-Layer Perceptron.\n",
    "\n",
    "- Mehrere Schichten.\n",
    "- Nichtlineare Aktivierungsfunktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        \n",
    "        self.fc_1 = FullyConnectedLayer(512)\n",
    "        self.relu = ReluLayer()\n",
    "        self.fc_2 = FullyConnectedLayer(10)\n",
    "        self.softmax = SoftmaxLayer()\n",
    "\n",
    "    def build_graph(self, input_shape): \n",
    "        input_shape_nobatch = input_shape[1:]\n",
    "        self.build(input_shape)\n",
    "        inputs = tf.keras.Input(shape=input_shape_nobatch)\n",
    "        \n",
    "        if not hasattr(self, 'call'):\n",
    "            raise AttributeError(\"User should define 'call' method in sub-class model!\")\n",
    "        \n",
    "        _ = self.call(inputs)\n",
    "    \n",
    "    def call(self, x):\n",
    "        y = self.fc_1(x)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc_2(y)\n",
    "        y = self.softmax(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Wir instantieren das tiefe Neuronale Netz mit 784 Eingangsneuronen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron()\n",
    "model.build_graph(input_shape=(None, 784,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Daten kodieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_images.astype(\"float32\").reshape((-1, 784)) / 255.0\n",
    "x_test = test_images.astype(\"float32\").reshape((-1, 784)) / 255.0\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(train_labels)\n",
    "y_test = to_categorical(test_labels)\n",
    "\n",
    "print(\"x_train:\", x_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"x_test:\", x_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Dataset-Visualisierung (kodiert):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in zip(x_train[:4], y_train[:4]):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Input.\")\n",
    "    plt.plot(image)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Output.\")\n",
    "    plt.plot(label)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Das Modell kompilieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Wie gut ist das Neuronale Netz vor dem Training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randint(0, len(x_test) - 1)\n",
    "image, label = x_test[index], y_test[index]\n",
    "prediction = model.predict(np.array([image]))[0]\n",
    "loss, accuracy = model.evaluate(np.array([image]), np.array([label]))\n",
    "\n",
    "plt.plot(prediction, label=\"Prediction\")\n",
    "plt.plot(label, label=\"Truth\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss: {} Accuracy: {}\".format(loss, accuracy))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Loss und Accuracy auf dem ganzen Dataset (vor dem Training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Loss: {} Accuracy: {}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Model-Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_train, y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Frage:** Wie funktioniert das Training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Auswertung der Trainings-Historie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Wie gut ist das Neuronale Netz nach dem Training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randint(0, len(x_test) - 1)\n",
    "image, label = x_test[index], y_test[index]\n",
    "prediction = model.predict(np.array([image]))[0]\n",
    "loss, accuracy = model.evaluate(np.array([image]), np.array([label]))\n",
    "\n",
    "plt.plot(prediction, label=\"Prediction\")\n",
    "plt.plot(label, label=\"Truth\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss: {} Accuracy: {}\".format(loss, accuracy))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Loss und Accuracy auf dem ganzen Dataset (nach dem Training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Loss: {} Accuracy: {}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Zusammenfassung, Hausaufgaben, Vorschau.\n",
    "\n",
    "### Zusammenfassung.\n",
    "- Tiefe Neuronale Netze sind gestapelte (nicht-lineare) Funktionen.\n",
    "- Training ist das algorithmische Ermitteln optimaler Freiheitsgrade.\n",
    "- Die Datengrundlage ist entscheidend.\n",
    "\n",
    "### Hausaufgaben.\n",
    "- [ ] Experimentieren mit der Implementierung.\n",
    "- [ ] Auf CIFAR10 und CIFAR100 trainieren.\n",
    "\n",
    "### Vorschau.\n",
    "- Weitere Schichten: Convolutions, Poolings, Recurrent Layers et cetera.\n",
    "- Im Detail: Stochastic Gradient Descent.\n",
    "- Wichtige Begriffe: Underfitting und Overfitting?\n",
    "- Use Cases: Unter anderem Image Processing, Natural Language Processing und Time Series Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen.\n",
    "\n",
    "- https://www.tensorflow.org\n",
    "- https://www.manning.com/books/deep-learning-with-python\n",
    "- https://www.deeplearningbook.org"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
